{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch book to try out different functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Read lines in move_lines to build a dict of lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import TextReader\n",
    "from word2vec import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"./Data/cornell_data/movie_lines.txt\"\n",
    "reader = TextReader()\n",
    "reader.read_line_dict(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first line stored in movie_lines.txt\n",
    "print(reader.lines[1045])\n",
    "# Find the longest line (used in seq2seq for attention)\n",
    "splitter = word2vec()\n",
    "max_length = 0\n",
    "max_line   = None\n",
    "max_split  = None\n",
    "for key in reader.lines.keys():\n",
    "    line = reader.lines[key]\n",
    "    length = len(list(splitter.sentence_to_list(line)))\n",
    "    if length > max_length:\n",
    "        max_length = length\n",
    "        max_line   = line\n",
    "        max_split  = splitter.sentence_to_list(line)\n",
    "\n",
    "print((max_length, max_line, max_split))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Get vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "vocab = defaultdict(int)\n",
    "for index in reader.lines.keys():\n",
    "    line = reader.lines[index]\n",
    "    words = line.split()\n",
    "    for word in words:\n",
    "        vocab[word] += 1\n",
    "\n",
    "print(len(vocab))\n",
    "print(vocab[b'They'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Read dialogues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default file path is movie_conversations\n",
    "reader.read_dialogues()\n",
    "print(reader.dialogues[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sanity Test on Word2Vec Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import TextReader\n",
    "from word2vec import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity test on training\n",
    "reader = TextReader()\n",
    "reader.read_line_dict(\"./Data/cornell_data/shortened_lines.txt\")\n",
    "\n",
    "word_model = word2vec()\n",
    "word_model.add_whole_corpus(reader)\n",
    "word_model.generate_indices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word_model.train(reader, epochs=1)\n",
    "word_model.merge_embeddings()\n",
    "word_model.save_embedding(\"word_embedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_model.load_embedding(\"word_embedding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Sanity Test for EncoderRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA\n",
      "Using CUDA for Seq2Seq\n"
     ]
    }
   ],
   "source": [
    "from seq2seq import EncoderRNN, AttnDecoderRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "HIDDEN_SIZE = 5\n",
    "INPUT_SIZE  = 12\n",
    "BATCH_SIZE  = 10\n",
    "\n",
    "\n",
    "encoder = EncoderRNN(input_size=INPUT_SIZE, hidden_size=HIDDEN_SIZE)\n",
    "decoder = AttnDecoderRNN(hidden_size=HIDDEN_SIZE, output_size=INPUT_SIZE, batch_size=BATCH_SIZE)\n",
    "\n",
    "# sequence length 5\n",
    "input_tensor = torch.randn(5, BATCH_SIZE, INPUT_SIZE)\n",
    "output = encoder(input_tensor, encoder.initHidden(batch_size=BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoding_start = torch.randn(1, BATCH_SIZE, INPUT_SIZE)\n",
    "decoded = decoder.forward(decoding_start, decoder.initHidden(), output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[-2.4986, -2.7907, -2.2938, -2.2136, -2.7745, -2.5925, -2.6909, -2.6268,\n",
      "         -2.1018, -2.6232, -2.5195, -2.3690],\n",
      "        [-2.5788, -2.7653, -2.3102, -2.3375, -2.8125, -2.5603, -2.5890, -2.5787,\n",
      "         -2.0528, -2.6005, -2.4524, -2.4263],\n",
      "        [-2.5695, -2.7711, -2.3112, -2.3230, -2.8060, -2.5636, -2.6005, -2.5809,\n",
      "         -2.0564, -2.6024, -2.4600, -2.4207],\n",
      "        [-2.5339, -2.7894, -2.3199, -2.2594, -2.7971, -2.5826, -2.6340, -2.5877,\n",
      "         -2.0659, -2.5890, -2.4871, -2.4304],\n",
      "        [-2.5752, -2.7189, -2.3072, -2.3212, -2.8537, -2.5738, -2.5613, -2.5821,\n",
      "         -2.0614, -2.5451, -2.4526, -2.5047],\n",
      "        [-2.5597, -2.7779, -2.3144, -2.3125, -2.7972, -2.5640, -2.6131, -2.5808,\n",
      "         -2.0596, -2.6071, -2.4674, -2.4123],\n",
      "        [-2.5618, -2.7771, -2.3156, -2.3169, -2.7978, -2.5624, -2.6104, -2.5788,\n",
      "         -2.0582, -2.6069, -2.4656, -2.4136],\n",
      "        [-2.5312, -2.7828, -2.2967, -2.2657, -2.7874, -2.5771, -2.6526, -2.6107,\n",
      "         -2.0820, -2.6220, -2.4914, -2.3798],\n",
      "        [-2.6321, -2.6978, -2.2990, -2.4249, -2.8605, -2.5434, -2.5141, -2.5680,\n",
      "         -2.0471, -2.5727, -2.4128, -2.4839],\n",
      "        [-2.5383, -2.7862, -2.3122, -2.2696, -2.7914, -2.5791, -2.6361, -2.5919,\n",
      "         -2.0699, -2.6011, -2.4857, -2.4123]], grad_fn=<LogSoftmaxBackward>), (tensor([[[ 8.2165e-02, -7.4991e-02, -1.0801e-01,  1.9007e-01,  1.8988e-01],\n",
      "         [ 3.4132e-02,  8.2975e-02,  7.1337e-02,  5.4765e-02,  1.5681e-01],\n",
      "         [ 3.1451e-02,  6.3480e-02,  5.2681e-02,  6.2896e-02,  1.5808e-01],\n",
      "         [ 4.3084e-02, -3.3964e-02, -1.3321e-02,  8.2806e-02,  2.1108e-01],\n",
      "         [ 1.5260e-01,  2.0242e-02,  9.5719e-02,  8.1758e-02,  2.8130e-01],\n",
      "         [ 2.3134e-02,  4.8637e-02,  3.7049e-02,  6.4504e-02,  1.4850e-01],\n",
      "         [ 2.0852e-02,  5.3874e-02,  4.3196e-02,  5.9090e-02,  1.4515e-01],\n",
      "         [ 5.6663e-02,  9.3280e-05, -4.1644e-02,  1.3899e-01,  1.6320e-01],\n",
      "         [ 1.1501e-01,  1.7667e-01,  2.0123e-01,  4.2435e-02,  1.7885e-01],\n",
      "         [ 3.9929e-02, -1.1828e-02, -1.1584e-02,  9.8816e-02,  1.8541e-01]]],\n",
      "       grad_fn=<StackBackward>), tensor([[[ 1.2979e-01, -1.1840e-01, -1.8823e-01,  3.4831e-01,  3.0764e-01],\n",
      "         [ 5.8495e-02,  1.4955e-01,  1.1970e-01,  1.1590e-01,  2.3861e-01],\n",
      "         [ 5.4430e-02,  1.1231e-01,  8.9005e-02,  1.3371e-01,  2.4026e-01],\n",
      "         [ 7.4255e-02, -6.1831e-02, -2.1616e-02,  1.7514e-01,  3.0963e-01],\n",
      "         [ 2.2045e-01,  4.4218e-02,  1.2933e-01,  1.4074e-01,  4.1874e-01],\n",
      "         [ 4.0870e-02,  8.5384e-02,  6.3892e-02,  1.4067e-01,  2.2363e-01],\n",
      "         [ 3.7060e-02,  9.5193e-02,  7.4546e-02,  1.3019e-01,  2.1782e-01],\n",
      "         [ 9.1891e-02,  1.5555e-04, -7.3761e-02,  2.7117e-01,  2.5601e-01],\n",
      "         [ 1.7768e-01,  3.3248e-01,  3.2798e-01,  7.9147e-02,  2.8800e-01],\n",
      "         [ 6.8802e-02, -1.9934e-02, -1.9386e-02,  2.0311e-01,  2.8389e-01]]],\n",
      "       grad_fn=<StackBackward>)), tensor([[[0.1043, 0.1354, 0.0592,  ..., 0.1129, 0.1923, 0.1016],\n",
      "         [0.0349, 0.0839, 0.0902,  ..., 0.0338, 0.0701, 0.1231],\n",
      "         [0.0732, 0.1271, 0.0856,  ..., 0.0768, 0.0980, 0.1244],\n",
      "         ...,\n",
      "         [0.1847, 0.2016, 0.1324,  ..., 0.1017, 0.2127, 0.1253],\n",
      "         [0.0526, 0.0814, 0.1580,  ..., 0.0571, 0.0512, 0.0705],\n",
      "         [0.1978, 0.0872, 0.1163,  ..., 0.2000, 0.1091, 0.1001]]],\n",
      "       grad_fn=<SoftmaxBackward>))\n"
     ]
    }
   ],
   "source": [
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
