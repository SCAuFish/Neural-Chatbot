# Log for the Project
In order to manage this project and record possible ideas, I create this
document to better organize (hopefully).

## May. 7th 2019
Decided to work on the neural chatbot frame work proposed by Jiwei Li et al in Deep Reinforcement Learning for Dialogue Generation. Other interesting topics involve target-based conversation agent, like the one based on POMDP proposed by Jason Williams et al in Partially Observable Markov Decision Processes for Spoken Dialog Systems.
Today try to collect data used in the paper--OpenSubtitles dataset. And it's crazy because it's large and not clean.
Therefore I am using the Cornell dataset given here: http://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html to try training a seq2seq model first.
It's much smaller. Let's write a simple reader for it.

## May. 20th 2019
It's been 13 days since last time I did this!
It's too late tonight and I finished the reader which can read line
by line.
read_dialogues not tested yet

## May. 21st 2019
Based on the tutorial from https://towardsdatascience.com/implementing-word2vec-in-pytorch-skip-gram-model-e6bae040d2fb, implemented a word2vec model based on skip-gram algorithm. Glad that I have learned this in CSE156. But I still have to follow the tutorial all the way... Wo hao cai a
<TODO>: Notice that the strings read from movie_lines.txt are byte strings. Be sure to check this in the future.
The training code hasn't been tested yet. After that we should be good to start implementing seq2seq model.

## May. 25th 2019
Spent a whole night on adjusting word2vec to make it work on cuda...
But it now works! I couldn't make cuda10 compatible with torch so I have to install my 1070.
What a world!
Next we can start seq2seq finally. With the experiences, hopefully the debugging process may be much shorter.
Notes:
    requires_grad_() should be called after the tensor is transferred to cuda. Or else the new tensor does not have grad!

## May. 26th 2019
So I have implemented seq2seq model based on the tutorial. However since I am using lstm with attentive reader, the implementation has been a little different from the tutorial.
Especially the tutorial doesn't cover the case with multiple batches so I had to do bunch of experiments to make sure the dimensions match.
At last it works. Next we should train word2vec to get the embeddings, then try training seq2seq model.

## May. 28th 2019
Trained to the 22% and saved. Continue next time.

## May. 30th 2019
Trained to the 52% and saved. Continue next time.
Put baseline training into Jupyter notebook. Still have some running errors. Besides, there are some changes worth noticing that are different from the online tutorial.
1. Notice that the tutorial does not contain batch-related implementation. Therefore all matrices created in the methods need to consider multi-batch situation and have three dimensions (seq x batch x features).
2. While the tutorial is using one-hot encoding for words, we are using word2vec model. Therefore, the decoder output may be directly compared with target instead of transforming into one-hot encoding. Based on the same reason, the loss function chosen is Mean Square Error instead of NLL given.
3. My training method does not halt when the decoder outputs EOF if the target string is not finished. It may be hard to manage for batch and it also makes sense to keep learning the rest of the sentence.
TODO: Verify the training function.

## Jun. 8th 2019
--Time flies like threads and runs away like a horse.
Improvement today:
1. Made use of word2vec from gensim so that I don't have to worry about word-embedding training anymore.
2. pure_movie_lines is made with all words lower cased, with START and EOS tokens inserted. word2vec should handle all sentence in the same way.
New ingredients in the model:
Since the training method is verified, we may start working on policy gradient (can you believe I start on that right before the due date ????)
1. Reward functions that evaluate a conversation generated by a chatbot is in reward.py. It is a wrapper model inheriting from nn.module so that we don't have to worry about calculating the gradients! Genius idea!
2. A trouble now is that two of the three reward function components require probability from the model. But we don't have that as a neural network! The paper says they implemented in a "stochastic representation" of the policy. Don't know what that means.
If it is too complicated to introduce such a representation, I may have to skip those two components and use a small epsilon as the probability when calculating the final reward

## Jun. 9th 2019
--Why people only improve their efficiency before deadlines?
Both Seq2Seq models and chatbot are trainable now. Simply run "python3 chatbot.py" and "python3 seq2seq.py". Have done some trial running but still did not know the results.
There are still two problems pending resolving for now:
1. How to implement a probabilistic model so that policy gradient is possible?
2. Is it correct to implement the reward function as a wrapper layer? Does the conversation generation need to be no_grad? I think no because otherwise the gradients are unable to pass into the encoder/decoder models.